<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"
  xmlns:atom="http://www.w3.org/2005/Atom"
  xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Thinker&#39;s</title>
    <link>http://Molldy.github.io/</link>
    <atom:link href="/rss2.xml" rel="self" type="application/rss+xml"/>
    
    <description></description>
    <pubDate>Fri, 13 Apr 2018 08:57:23 GMT</pubDate>
    <generator>http://hexo.io/</generator>
    
    <item>
      <title>AlexNet训练MNIST</title>
      <link>http://Molldy.github.io/2018/04/13/AlexNet%E8%AE%AD%E7%BB%83MNIST/</link>
      <guid>http://Molldy.github.io/2018/04/13/AlexNet%E8%AE%AD%E7%BB%83MNIST/</guid>
      <pubDate>Fri, 13 Apr 2018 08:54:36 GMT</pubDate>
      <description>
      
        
        
          &lt;p&gt;基于AlexNet模型，Mnist数据集进行修改（3conventional layers + 3full_connect layers）&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;
        
      
      </description>
      
      <content:encoded><![CDATA[<p>基于AlexNet模型，Mnist数据集进行修改（3conventional layers + 3full_connect layers）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># import</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> tensorflow.examples.tutorials.mnist.input_data <span class="keyword">as</span> input_data</span><br><span class="line"></span><br><span class="line"><span class="comment"># data</span></span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">'MNIST_data/'</span>, one_hot=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># hyperparameters &amp; parameters</span></span><br><span class="line"><span class="comment"># define hyperparameters</span></span><br><span class="line">learning_rate = <span class="number">0.001</span></span><br><span class="line">training_iters = <span class="number">200000</span></span><br><span class="line">batch_size = <span class="number">64</span></span><br><span class="line">display_step = <span class="number">20</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># define network parameters</span></span><br><span class="line">n_input = <span class="number">784</span> <span class="comment"># 输入的维度</span></span><br><span class="line">n_classes = <span class="number">10</span> <span class="comment"># 标签的维度</span></span><br><span class="line">dropout = <span class="number">0.8</span> <span class="comment"># Dropout 的概率</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># placeholder</span></span><br><span class="line">x = tf.placeholder(tf.float32, [<span class="keyword">None</span>, n_input])</span><br><span class="line">y = tf.placeholder(tf.float32, [<span class="keyword">None</span>, n_classes])</span><br><span class="line">keep_prob = tf.placeholder(tf.float32)</span><br><span class="line"></span><br><span class="line"><span class="comment"># define add_layers</span></span><br><span class="line"><span class="comment"># conv2d</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv2d</span><span class="params">(name, l_input, w, b)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(l_input, w, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>),b), name=name)</span><br><span class="line"></span><br><span class="line"><span class="comment"># max_pool</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">max_pool</span><span class="params">(name, l_input, k)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.nn.max_pool(l_input, ksize=[<span class="number">1</span>, k, k, <span class="number">1</span>], strides=[<span class="number">1</span>, k, k, <span class="number">1</span>], padding=<span class="string">'SAME'</span>, name=name)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 归一化操作</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">norm</span><span class="params">(name, l_input, lsize=<span class="number">4</span>)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.nn.lrn(l_input, lsize, bias=<span class="number">1.0</span>, alpha=<span class="number">0.001</span> / <span class="number">9.0</span>, beta=<span class="number">0.75</span>, name=name)</span><br><span class="line"></span><br><span class="line"><span class="comment"># alex_net</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">alex_net</span><span class="params">(_X, _weights, _biases, _dropout)</span>:</span></span><br><span class="line">    <span class="comment"># 向量转为矩阵</span></span><br><span class="line">    _X = tf.reshape(_X, shape=[<span class="number">-1</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># conv1_layer</span></span><br><span class="line">    conv1 = conv2d(<span class="string">'conv1'</span>, _X, _weights[<span class="string">'wc1'</span>], _biases[<span class="string">'bc1'</span>])</span><br><span class="line">    pool1 = max_pool(<span class="string">'pool1'</span>, conv1, k=<span class="number">2</span>)</span><br><span class="line">    norm1 = norm(<span class="string">'norm1'</span>, pool1, lsize=<span class="number">4</span>)      <span class="comment"># 归一化</span></span><br><span class="line">    norm1 = tf.nn.dropout(norm1, _dropout)     <span class="comment"># Dropout</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># conv2_layer</span></span><br><span class="line">    conv2 = conv2d(<span class="string">'conv2'</span>, norm1, _weights[<span class="string">'wc2'</span>], _biases[<span class="string">'bc2'</span>])</span><br><span class="line">    pool2 = max_pool(<span class="string">'pool2'</span>, conv2, k=<span class="number">2</span>)</span><br><span class="line">    norm2 = norm(<span class="string">'norm2'</span>, pool2, lsize=<span class="number">4</span>)</span><br><span class="line">    norm2 = tf.nn.dropout(norm2, _dropout)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># conv3_layer</span></span><br><span class="line">    conv3 = conv2d(<span class="string">'conv3'</span>, norm2, _weights[<span class="string">'wc3'</span>], _biases[<span class="string">'bc3'</span>])</span><br><span class="line">    pool3 = max_pool(<span class="string">'pool3'</span>, conv3, k=<span class="number">2</span>)</span><br><span class="line">    norm3 = norm(<span class="string">'norm3'</span>, pool3, lsize=<span class="number">4</span>)</span><br><span class="line">    norm3 = tf.nn.dropout(norm3, _dropout)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># func1_layer</span></span><br><span class="line">    dense1 = tf.reshape(norm3, [<span class="number">-1</span>, _weights[<span class="string">'wd1'</span>].get_shape().as_list()[<span class="number">0</span>]])</span><br><span class="line">    dense1 = tf.nn.relu(tf.matmul(dense1, _weights[<span class="string">'wd1'</span>]) + _biases[<span class="string">'bd1'</span>], name=<span class="string">'fc1'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># func2_layer</span></span><br><span class="line">    dense2 = tf.nn.relu(tf.matmul(dense1, _weights[<span class="string">'wd2'</span>]) + _biases[<span class="string">'bd2'</span>], name=<span class="string">'fc2'</span>) <span class="comment"># Relu activation</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># out_layer</span></span><br><span class="line">    out = tf.matmul(dense2, _weights[<span class="string">'out'</span>]) + _biases[<span class="string">'out'</span>]</span><br><span class="line">    <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"><span class="comment"># add_layers</span></span><br><span class="line">weights = &#123;</span><br><span class="line">    <span class="string">'wc1'</span>: tf.Variable(tf.random_normal([<span class="number">3</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">64</span>])),</span><br><span class="line">    <span class="string">'wc2'</span>: tf.Variable(tf.random_normal([<span class="number">3</span>, <span class="number">3</span>, <span class="number">64</span>, <span class="number">128</span>])),</span><br><span class="line">    <span class="string">'wc3'</span>: tf.Variable(tf.random_normal([<span class="number">3</span>, <span class="number">3</span>, <span class="number">128</span>, <span class="number">256</span>])),</span><br><span class="line">    <span class="string">'wd1'</span>: tf.Variable(tf.random_normal([<span class="number">4</span>*<span class="number">4</span>*<span class="number">256</span>, <span class="number">1024</span>])),</span><br><span class="line">    <span class="string">'wd2'</span>: tf.Variable(tf.random_normal([<span class="number">1024</span>, <span class="number">1024</span>])),</span><br><span class="line">    <span class="string">'out'</span>: tf.Variable(tf.random_normal([<span class="number">1024</span>, <span class="number">10</span>]))</span><br><span class="line">&#125;</span><br><span class="line">biases = &#123;</span><br><span class="line">    <span class="string">'bc1'</span>: tf.Variable(tf.random_normal([<span class="number">64</span>])),</span><br><span class="line">    <span class="string">'bc2'</span>: tf.Variable(tf.random_normal([<span class="number">128</span>])),</span><br><span class="line">    <span class="string">'bc3'</span>: tf.Variable(tf.random_normal([<span class="number">256</span>])),</span><br><span class="line">    <span class="string">'bd1'</span>: tf.Variable(tf.random_normal([<span class="number">1024</span>])),</span><br><span class="line">    <span class="string">'bd2'</span>: tf.Variable(tf.random_normal([<span class="number">1024</span>])),</span><br><span class="line">    <span class="string">'out'</span>: tf.Variable(tf.random_normal([n_classes]))</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">pred = alex_net(x, weights, biases, keep_prob)</span><br><span class="line"></span><br><span class="line"><span class="comment"># loss_function</span></span><br><span class="line">cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = pred, labels = y))</span><br><span class="line"></span><br><span class="line"><span class="comment"># train_step</span></span><br><span class="line">optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Session</span></span><br><span class="line"><span class="comment"># define accuracy</span></span><br><span class="line">correct_pred = tf.equal(tf.argmax(pred,<span class="number">1</span>), tf.argmax(y,<span class="number">1</span>))</span><br><span class="line">accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))</span><br><span class="line"></span><br><span class="line">init = tf.initialize_all_variables()</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    step = <span class="number">1</span></span><br><span class="line">    <span class="comment"># Keep training until reach max iterations</span></span><br><span class="line">    <span class="keyword">while</span> step * batch_size &lt; training_iters:</span><br><span class="line">        batch_xs, batch_ys = mnist.train.next_batch(batch_size)</span><br><span class="line">        <span class="comment"># 获取批数据</span></span><br><span class="line">        sess.run(optimizer, feed_dict=&#123;x: batch_xs, y: batch_ys, keep_prob: dropout&#125;)</span><br><span class="line">        <span class="keyword">if</span> step % display_step == <span class="number">0</span>:</span><br><span class="line">            <span class="comment"># 计算精度</span></span><br><span class="line">            acc = sess.run(accuracy, feed_dict=&#123;x: batch_xs, y: batch_ys, keep_prob: <span class="number">1.</span>&#125;)</span><br><span class="line">            <span class="comment"># 计算损失值</span></span><br><span class="line">            loss = sess.run(cost, feed_dict=&#123;x: batch_xs, y: batch_ys, keep_prob: <span class="number">1.</span>&#125;)</span><br><span class="line">            <span class="keyword">print</span> (<span class="string">"Iter "</span> + str(step*batch_size) + <span class="string">", Minibatch Loss= "</span> + <span class="string">"&#123;:.6f&#125;"</span>.format(loss) + <span class="string">", Training Accuracy = "</span> + <span class="string">"&#123;:.5f&#125;"</span>.format(acc))</span><br><span class="line">        step += <span class="number">1</span></span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"Optimization Finished!"</span>)</span><br><span class="line">    <span class="comment"># 计算测试精度</span></span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"Testing Accuracy:"</span>, sess.run(accuracy, feed_dict=&#123;x: mnist.test.images[:<span class="number">256</span>], y: mnist.test.labels[:<span class="number">256</span>], keep_prob: <span class="number">1.</span>&#125;))</span><br></pre></td></tr></table></figure>]]></content:encoded>
      
      <comments>http://Molldy.github.io/2018/04/13/AlexNet%E8%AE%AD%E7%BB%83MNIST/#disqus_thread</comments>
    </item>
    
    <item>
      <title>基于MNIST数据集的LeNet-5模型实现</title>
      <link>http://Molldy.github.io/2018/04/13/%E5%9F%BA%E4%BA%8EMNIST%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84LeNet-5%E6%A8%A1%E5%9E%8B%E5%AE%9E%E7%8E%B0/</link>
      <guid>http://Molldy.github.io/2018/04/13/%E5%9F%BA%E4%BA%8EMNIST%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84LeNet-5%E6%A8%A1%E5%9E%8B%E5%AE%9E%E7%8E%B0/</guid>
      <pubDate>Fri, 13 Apr 2018 08:11:56 GMT</pubDate>
      <description>
      
        
        
          &lt;h2 id=&quot;基于MNIST数据集的LeNet-5模型实现&quot;&gt;&lt;a href=&quot;#基于MNIST数据集的LeNet-5模型实现&quot; class=&quot;headerlink&quot; title=&quot;基于MNIST数据集的LeNet-5模型实现&quot;&gt;&lt;/a&gt;基于MNIST数据集的LeNet-5模型
        
      
      </description>
      
      <content:encoded><![CDATA[<h2 id="基于MNIST数据集的LeNet-5模型实现"><a href="#基于MNIST数据集的LeNet-5模型实现" class="headerlink" title="基于MNIST数据集的LeNet-5模型实现"></a>基于MNIST数据集的LeNet-5模型实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># import</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> tensorflow.examples.tutorials.mnist.input_data <span class="keyword">as</span> input_data</span><br><span class="line"></span><br><span class="line"><span class="comment"># data</span></span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">'MNIST_data/'</span>, one_hot=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># def Variables&amp;add_layer</span></span><br><span class="line"><span class="comment">#   定义准确率输出函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_accuracy</span><span class="params">(v_xs, v_ys)</span>:</span></span><br><span class="line">    <span class="keyword">global</span> prediction</span><br><span class="line">    y_pre = sess.run(prediction, feed_dict=&#123;xs: v_xs, keep_prob: <span class="number">1</span>&#125;)</span><br><span class="line">    correct_prediction = tf.equal(tf.argmax(y_pre, <span class="number">1</span>), tf.argmax(v_ys, <span class="number">1</span>)) <span class="comment"># 找出预测正确的标签</span></span><br><span class="line">    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) <span class="comment"># 得出通过正确个数除以总数得出准确率</span></span><br><span class="line">    result = sess.run(accuracy, feed_dict=&#123;xs: v_xs, ys: v_ys, keep_prob: <span class="number">1</span>&#125;)</span><br><span class="line">    <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Weights_variable</span><span class="params">(shape)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.Variable(tf.truncated_normal(shape, stddev=<span class="number">0.1</span>))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">biases_variable</span><span class="params">(shape)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.Variable(tf.zeros(shape) + <span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv2d</span><span class="params">(x, W)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.nn.conv2d(x, W, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">max_pool</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.nn.max_pool(x, ksize=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># placeholder</span></span><br><span class="line">xs = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">784</span>])</span><br><span class="line">ys = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">10</span>])</span><br><span class="line">xs_image = tf.reshape(xs, [<span class="number">-1</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>]) <span class="comment"># [batch, in_size, out_size, in_channels]</span></span><br><span class="line">keep_prob = tf.placeholder(tf.float32)  <span class="comment"># dropout层参数</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># add layers</span></span><br><span class="line"><span class="comment"># conv1 layer</span></span><br><span class="line">W_conv1 = Weights_variable([<span class="number">5</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">32</span>]) <span class="comment"># [filter_height, filter_width, in_channels, out_channels]</span></span><br><span class="line">b_conv1 = biases_variable([<span class="number">32</span>])</span><br><span class="line">h_conv1 = tf.nn.relu(conv2d(xs_image, W_conv1) + b_conv1)</span><br><span class="line">h_pool1 = max_pool(h_conv1) <span class="comment"># pooling layer, out_size=14x14x32</span></span><br><span class="line"><span class="comment"># conv2 layer</span></span><br><span class="line">W_conv2 = Weights_variable([<span class="number">5</span>, <span class="number">5</span>, <span class="number">32</span>, <span class="number">64</span>]) <span class="comment"># [filter_height, filter_width, in_channels, out_channels]</span></span><br><span class="line">b_conv2 = biases_variable([<span class="number">64</span>])</span><br><span class="line">h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)</span><br><span class="line">h_pool2 = max_pool(h_conv2) <span class="comment"># pooling layer, out_size=7x7x64</span></span><br><span class="line"><span class="comment"># func1 layer</span></span><br><span class="line">W_func1 = Weights_variable([<span class="number">7</span>*<span class="number">7</span>*<span class="number">64</span>, <span class="number">1024</span>])</span><br><span class="line">b_func1 = biases_variable([<span class="number">1024</span>])</span><br><span class="line">h_pool2_flat = tf.reshape(h_pool2, [<span class="number">-1</span>, <span class="number">7</span>*<span class="number">7</span>*<span class="number">64</span>]) <span class="comment"># 将h_pool2变成1维的数据</span></span><br><span class="line">h_func1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_func1) + b_func1)</span><br><span class="line">h_func1_drop = tf.nn.dropout(h_func1, keep_prob) <span class="comment"># dropout layer</span></span><br><span class="line"><span class="comment"># func2 layer</span></span><br><span class="line">W_func2 = Weights_variable([<span class="number">1024</span>, <span class="number">10</span>])</span><br><span class="line">b_func2 = biases_variable([<span class="number">10</span>])</span><br><span class="line">prediction = tf.nn.softmax(tf.matmul(h_func1_drop, W_func2) + b_func2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># loss function</span></span><br><span class="line">cross_entropy = tf.reduce_mean(-tf.reduce_sum(ys * tf.log(prediction),</span><br><span class="line">                                              reduction_indices=[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># train_step</span></span><br><span class="line">train_step = tf.train.AdamOptimizer(<span class="number">1e-4</span>).minimize(cross_entropy)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Session</span></span><br><span class="line">sess = tf.Session()</span><br><span class="line">sess.run(tf.initialize_all_variables())</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">    batch_xs, batch_ys = mnist.train.next_batch(<span class="number">100</span>)</span><br><span class="line">    sess.run(train_step, feed_dict=&#123;xs: batch_xs, ys: batch_ys, keep_prob: <span class="number">0.5</span>&#125;)</span><br><span class="line">    <span class="keyword">if</span> i % <span class="number">20</span> == <span class="number">0</span>:</span><br><span class="line">        print(compute_accuracy(</span><br><span class="line">            mnist.test.images, mnist.test.labels))</span><br></pre></td></tr></table></figure>]]></content:encoded>
      
      <comments>http://Molldy.github.io/2018/04/13/%E5%9F%BA%E4%BA%8EMNIST%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84LeNet-5%E6%A8%A1%E5%9E%8B%E5%AE%9E%E7%8E%B0/#disqus_thread</comments>
    </item>
    
    <item>
      <title>简单2层神经网络模型实现</title>
      <link>http://Molldy.github.io/2018/04/13/%E7%AE%80%E5%8D%952%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E5%AE%9E%E7%8E%B0/</link>
      <guid>http://Molldy.github.io/2018/04/13/%E7%AE%80%E5%8D%952%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E5%AE%9E%E7%8E%B0/</guid>
      <pubDate>Fri, 13 Apr 2018 08:06:02 GMT</pubDate>
      <description>
      
        
        
          &lt;h2 id=&quot;简单2层神经网络模型实现&quot;&gt;&lt;a href=&quot;#简单2层神经网络模型实现&quot; class=&quot;headerlink&quot; title=&quot;简单2层神经网络模型实现&quot;&gt;&lt;/a&gt;简单2层神经网络模型实现&lt;/h2&gt;&lt;figure class=&quot;highlight python&quot;&gt;
        
      
      </description>
      
      <content:encoded><![CDATA[<h2 id="简单2层神经网络模型实现"><a href="#简单2层神经网络模型实现" class="headerlink" title="简单2层神经网络模型实现"></a>简单2层神经网络模型实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># import</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># data</span></span><br><span class="line">x_data = np.linspace(<span class="number">-1</span>,<span class="number">1</span>,<span class="number">300</span>)[:, np.newaxis]  <span class="comment">#-1~1,300个单位</span></span><br><span class="line">noise = np.random.normal(<span class="number">0</span>, <span class="number">0.05</span>, x_data.shape)  <span class="comment">#添加噪声，正态分布生成随机数（方差是0.05，格式是x_data.shape）</span></span><br><span class="line">y_data = np.square(x_data) - <span class="number">0.5</span> + noise  <span class="comment">#y=x^2-0.5</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># def Variables &amp; add_layer</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_layer</span><span class="params">(inputs, in_size, out_size, activation_function = None)</span>:</span></span><br><span class="line">    Weights = tf.Variable(tf.random_normal([in_size, out_size], stddev=<span class="number">0.1</span>))</span><br><span class="line">    biases = tf.Variable(tf.zeros([out_size]) + <span class="number">0.1</span>)</span><br><span class="line">    Wx_plus_b = tf.matmul(inputs, Weights) + biases</span><br><span class="line">    <span class="keyword">if</span> activation_function <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">        output = Wx_plus_b</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        output = activation_function(Wx_plus_b)</span><br><span class="line">    <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"><span class="comment"># define placeholder</span></span><br><span class="line">xs = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">1</span>])</span><br><span class="line">ys = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">1</span>])</span><br><span class="line"><span class="comment"># add layers</span></span><br><span class="line">l1 = add_layer(xs, <span class="number">1</span>, <span class="number">10</span>, activation_function=tf.nn.relu)</span><br><span class="line">prediction = add_layer(l1, <span class="number">10</span>, <span class="number">1</span>, activation_function=<span class="keyword">None</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># loss function</span></span><br><span class="line">loss = tf.reduce_mean(tf.reduce_sum(tf.square(prediction - ys), reduction_indices=[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># train</span></span><br><span class="line">train_step = tf.train.GradientDescentOptimizer(<span class="number">0.5</span>).minimize(loss)</span><br><span class="line"></span><br><span class="line"><span class="comment"># session</span></span><br><span class="line">sess = tf.Session()</span><br><span class="line">sess.run(tf.initialize_all_variables())</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">    sess.run(train_step, feed_dict=&#123;xs: x_data, ys: y_data&#125;)</span><br><span class="line">    <span class="keyword">if</span> i % <span class="number">50</span> == <span class="number">0</span>:</span><br><span class="line">        print(sess.run(loss, feed_dict=&#123;xs: x_data, ys: y_data&#125;))</span><br></pre></td></tr></table></figure>]]></content:encoded>
      
      <comments>http://Molldy.github.io/2018/04/13/%E7%AE%80%E5%8D%952%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E5%AE%9E%E7%8E%B0/#disqus_thread</comments>
    </item>
    
    <item>
      <title>欢迎</title>
      <link>http://Molldy.github.io/2018/04/07/%E6%AC%A2%E8%BF%8E/</link>
      <guid>http://Molldy.github.io/2018/04/07/%E6%AC%A2%E8%BF%8E/</guid>
      <pubDate>Sat, 07 Apr 2018 12:17:03 GMT</pubDate>
      <description>
      
        
        
          &lt;h2 id=&quot;Welcome-to-Hou’s-blog&quot;&gt;&lt;a href=&quot;#Welcome-to-Hou’s-blog&quot; class=&quot;headerlink&quot; title=&quot;Welcome to Hou’s blog&quot;&gt;&lt;/a&gt;Welcome to Hou’s blog&lt;/
        
      
      </description>
      
      <content:encoded><![CDATA[<h2 id="Welcome-to-Hou’s-blog"><a href="#Welcome-to-Hou’s-blog" class="headerlink" title="Welcome to Hou’s blog"></a>Welcome to Hou’s blog</h2><p>Hello，我是Molldy Hou，欢迎来到我的博客！</p><p>在这里，我会将我所学习到的知识整理下来。欢迎交流。</p>]]></content:encoded>
      
      <comments>http://Molldy.github.io/2018/04/07/%E6%AC%A2%E8%BF%8E/#disqus_thread</comments>
    </item>
    
    <item>
      <title>test</title>
      <link>http://Molldy.github.io/2018/04/07/test/</link>
      <guid>http://Molldy.github.io/2018/04/07/test/</guid>
      <pubDate>Sat, 07 Apr 2018 12:09:44 GMT</pubDate>
      <description>
      
        
        
          &lt;p&gt;hhhhh&lt;/p&gt;

        
      
      </description>
      
      <content:encoded><![CDATA[<p>hhhhh</p>]]></content:encoded>
      
      <comments>http://Molldy.github.io/2018/04/07/test/#disqus_thread</comments>
    </item>
    
  </channel>
</rss>
